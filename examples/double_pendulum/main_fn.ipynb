{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_fn(path_config_file, extra_args={}):\n",
    "    # Read the configration file\n",
    "    weight_noise, bias_noise, mSampleLog, mParamsNN_list, (type_baseline, data_set_file, out_file) = load_config_file(\n",
    "        path_config_file, extra_args)\n",
    "    reducedSampleLog = SampleLog(xTrain=None, xTrainExtra=None, uTrain=None, xnextTrain=None,\n",
    "                                 lowU_train=mSampleLog.lowU_train, highU_train=mSampleLog.highU_train,\n",
    "                                 xTest=None, xTestExtra=None, uTest=None, xnextTest=None,\n",
    "                                 lowU_test=mSampleLog.lowU_test, highU_test=mSampleLog.highU_test,\n",
    "                                 env_name=mSampleLog.env_name, env_extra_args=mSampleLog.env_extra_args,\n",
    "                                 m_rng=mSampleLog.m_rng, seed_number=mSampleLog.seed_number, qp_indx=mSampleLog.qp_indx,\n",
    "                                 qp_base=mSampleLog.qp_base, n_state=mSampleLog.n_state, n_control=mSampleLog.n_control,\n",
    "                                 actual_dt=mSampleLog.actual_dt, disable_substep=mSampleLog.disable_substep,\n",
    "                                 control_policy=mSampleLog.control_policy, n_rollout=mSampleLog.n_rollout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mSampleLog is literally just the train data file\n",
    "reducedSampleLog is like mSampleLog, but with less unecessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Print the sample log info\n",
    "    print(\"###### SampleLog ######\")\n",
    "    print(reducedSampleLog)\n",
    "\n",
    "    # Training data\n",
    "    xTrainList = np.asarray(mSampleLog.xTrain)\n",
    "    (xTrainExtra, coloc_set) = mSampleLog.xTrainExtra\n",
    "    xnextTrainList = np.asarray(mSampleLog.xnextTrain)\n",
    "    (_, num_traj_data, trajectory_length) = mSampleLog.disable_substep\n",
    "    coloc_set = jnp.array(coloc_set)\n",
    "\n",
    "    # Testing data\n",
    "    xTest = jnp.asarray(mSampleLog.xTest)\n",
    "    xTestNext = jnp.asarray(mSampleLog.xnextTest)\n",
    "\n",
    "    # DIctionary for logging training details\n",
    "    final_log_dict = {i: {} for i in range(len(num_traj_data))}\n",
    "    final_params_dict = {i: {} for i in range(len(num_traj_data))}\n",
    "\n",
    "    for i, n_train in tqdm(enumerate(num_traj_data),  total=len(num_traj_data)):\n",
    "        # The total number of point in this trajectory\n",
    "        total_traj_size = n_train*trajectory_length\n",
    "\n",
    "        # Dictionary to save the loss and the optimal params per radom seed\n",
    "        dict_loss_per_seed = {mParamsNN.seed_init: {}\n",
    "                              for mParamsNN in mParamsNN_list}\n",
    "        dict_params_per_seed = {\n",
    "            mParamsNN.seed_init: None for mParamsNN in mParamsNN_list}\n",
    "\n",
    "        for _, mParamsNN in tqdm(enumerate(mParamsNN_list), total=len(mParamsNN_list), leave=False):\n",
    "\n",
    "            # Some logging execution\n",
    "            dbg_msg = \"###### Hyper parameters: {} ######\\n\".format(\n",
    "                type_baseline)\n",
    "            dbg_msg += \"{}\\n\".format(mParamsNN)\n",
    "            tqdm.write(dbg_msg)\n",
    "\n",
    "            # Build the neural network, the update, loss function and paramters of the neural network structure\n",
    "            rng_gen, opt, (params, m_pen_eq_k, m_pen_ineq_k, m_lagr_eq_k, m_lagr_ineq_k), pred_xnext,\\\n",
    "                loss_fun, update, update_lagrange, loss_fun_constr, train_with_constraints = build_learner(\n",
    "                    mParamsNN, type_baseline)\n",
    "\n",
    "            # Initialize the optimizer with the initial parameters\n",
    "            opt_state = opt.init(params)\n",
    "\n",
    "            # Get the frequency at which to evaluate on the testing\n",
    "            high_freq_record_rg = int(\n",
    "                mParamsNN.freq_accuracy[0]*mParamsNN.num_gradient_iterations)\n",
    "            high_freq_val = mParamsNN.freq_accuracy[1]\n",
    "            low_freq_val = mParamsNN.freq_accuracy[2]\n",
    "            update_freq = jnp.array([(j % high_freq_val) == 0 if j <= high_freq_record_rg else ((j % low_freq_val) == 0 if j < mParamsNN.num_gradient_iterations-1 else True)\n",
    "                                     for j in range(mParamsNN.num_gradient_iterations)])\n",
    "\n",
    "            # Dictionary to save the loss, the rollout error, and the colocation accuracy\n",
    "            dict_loss = {'total_loss_train': list(), 'rollout_err_train': list(), 'total_loss_test': list(\n",
    "            ), 'rollout_err_test': list(), 'coloc_err_train': list(), 'coloc_err_test': list()}\n",
    "\n",
    "            # Store the best parameters attained by the learning process so far\n",
    "            best_params = None \t\t\t\t# Weights returning best loss over the training set\n",
    "            best_test_loss = None \t\t\t# Best test loss over the training set\n",
    "            best_train_loss = None \t\t\t# Best training loss\n",
    "            best_constr_val = None \t\t\t# Constraint attained by the best params\n",
    "            best_test_noconstr = None \t\t# Best mean squared error without any constraints\n",
    "            # Iteration since the last best weights values (iteration in terms of the loss evaluation period)\n",
    "            iter_since_best_param = None\n",
    "            number_lagrange_update = 0 \t\t# Current number of lagrangian and penalty terms updates\n",
    "            number_inner_iteration = 0 \t\t# Current number of iteration the latest update\n",
    "            # Period over which no improvement yields the best solution\n",
    "            patience = mParamsNN.patience\n",
    "            tqdm.write('\\t\\tEarly stopping criteria = {}'.format(patience > 0))\n",
    "\n",
    "            # Iterate to learn the weight of the neural network\n",
    "            for step in tqdm(range(mParamsNN.num_gradient_iterations), leave=False):\n",
    "\n",
    "                # Random key generator for batch data\n",
    "                rng_gen, subkey = jax.random.split(rng_gen)\n",
    "                idx_train = jax.random.randint(subkey, shape=(\n",
    "                    mParamsNN.batch_size,), minval=0, maxval=total_traj_size)\n",
    "\n",
    "                # Sample and put into the GPU memory the batch-size data\n",
    "                xTrain, xTrainNext = jnp.asarray(xTrainList[idx_train, :]), jnp.asarray(\n",
    "                    xnextTrainList[:, idx_train, :])\n",
    "\n",
    "                # Sample batches of the colocation data sets\n",
    "                rng_gen, subkey = jax.random.split(rng_gen)\n",
    "                idx_coloc_train = jax.random.randint(subkey, shape=(\n",
    "                    mParamsNN.batch_size,), minval=0, maxval=coloc_set.shape[0])\n",
    "                batch_coloc = coloc_set[idx_coloc_train]\n",
    "\n",
    "                # Update the parameters of the NN and the state of the optimizer\n",
    "                params, opt_state = update(params, opt_state, xTrainNext, xTrain, pen_eq_k=m_pen_eq_k, pen_ineq_sq_k=m_pen_ineq_k,\n",
    "                                           lagr_eq_k=m_lagr_eq_k if m_lagr_eq_k.shape[\n",
    "                                               0] <= 0 else m_lagr_eq_k[idx_coloc_train],\n",
    "                                           lagr_ineq_k=m_lagr_ineq_k if m_lagr_ineq_k.shape[\n",
    "                                               0] <= 0 else m_lagr_ineq_k[idx_coloc_train],\n",
    "                                           extra_args_colocation=(batch_coloc, None, None))\n",
    "\n",
    "                # If it is time to evaluate the models do it\n",
    "                if update_freq[number_inner_iteration]:\n",
    "                    if not train_with_constraints:  # In case there is no constraints -> Try to also log the constraints incurred by the current neural structure\n",
    "                        loss_tr, (spec_data_tr, coloc_ctr) = loss_fun_constr(\n",
    "                            params, xTrainNext, xTrain, gradient_regularization=gradient_regularization)\n",
    "                        loss_te, (spec_data_te, coloc_cte) = loss_fun_constr(\n",
    "                            params, xTestNext, xTest, extra_args_colocation=(coloc_set, None, None), gradient_regularization=gradient_regularization)\n",
    "                    else:\n",
    "                        loss_tr, (spec_data_tr, coloc_ctr) = loss_fun(\n",
    "                            params, xTrainNext, xTrain)\n",
    "                        loss_te, (spec_data_te, coloc_cte) = loss_fun(params, xTestNext, xTest, pen_eq_k=m_pen_eq_k, pen_ineq_sq_k=m_pen_ineq_k,\n",
    "                                                                      lagr_eq_k=m_lagr_eq_k, lagr_ineq_k=m_lagr_ineq_k,\n",
    "                                                                      extra_args_colocation=(coloc_set, None, None))\n",
    "\n",
    "                    # Log the value obtained by evaluating the current model\n",
    "                    dict_loss['total_loss_train'].append(float(loss_tr))\n",
    "                    dict_loss['total_loss_test'].append(float(loss_te))\n",
    "                    dict_loss['rollout_err_train'].append(spec_data_tr)\n",
    "                    dict_loss['rollout_err_test'].append(spec_data_te)\n",
    "                    dict_loss['coloc_err_train'].append(coloc_ctr)\n",
    "                    dict_loss['coloc_err_test'].append(coloc_cte)\n",
    "\n",
    "                    # Initialize the parameters for the best model so far\n",
    "                    if number_inner_iteration == 0:\n",
    "                        best_params = params\n",
    "                        best_test_noconstr = jnp.mean(\n",
    "                            spec_data_te[:, 1])  # Rollout mean\n",
    "                        best_test_loss = loss_te\n",
    "                        best_train_loss = loss_tr\n",
    "                        best_constr_val = coloc_cte\n",
    "                        iter_since_best_param = 0\n",
    "\n",
    "                    # Check if the validation metrics has improved\n",
    "                    if loss_te < best_test_loss:\n",
    "                        best_params = params\n",
    "                        best_test_loss = loss_te\n",
    "                        best_test_noconstr = jnp.mean(spec_data_te[:, 1])\n",
    "                        best_constr_val = coloc_cte\n",
    "                        best_train_loss = loss_tr if loss_tr < best_train_loss else best_train_loss\n",
    "                        iter_since_best_param = 0\n",
    "                    else:\n",
    "                        best_train_loss = loss_tr if loss_tr < best_train_loss else best_train_loss\n",
    "                        iter_since_best_param += 1\n",
    "\n",
    "                # Period at which we save the models\n",
    "                if number_inner_iteration % mParamsNN.freq_save == 0:\n",
    "\n",
    "                    # Update for the current seed what is the lost and the best params found so far\n",
    "                    dict_loss_per_seed[mParamsNN.seed_init] = dict_loss\n",
    "                    final_log_dict[i] = dict_loss_per_seed\n",
    "\n",
    "                    # Update the best params found so far\n",
    "                    dict_params_per_seed[mParamsNN.seed_init] = best_params\n",
    "                    final_params_dict[i] = dict_params_per_seed\n",
    "\n",
    "                    # Create the log file\n",
    "                    mLog = LearningLog(env_name=mSampleLog.env_name, env_extra_args=mSampleLog.env_extra_args, baseline=type_baseline,\n",
    "                                       sampleLog=reducedSampleLog, nn_hyperparams=mParamsNN_list, loss_evol=final_log_dict, learned_weights=final_params_dict,\n",
    "                                       data_set_file=data_set_file)\n",
    "\n",
    "                    # Save the current information in the file and close the file\n",
    "                    mFile = open(out_file+'.pkl', 'wb')\n",
    "                    pickle.dump(mLog, mFile)\n",
    "                    mFile.close()\n",
    "\n",
    "                    # Debug message to bring on the consolde\n",
    "                    dbg_msg = '[Iter[{}][{}], N={}]\\t train : {:.6f} | Loss test : {:.6f}\\n'.format(\n",
    "                        step, number_lagrange_update, n_train, loss_tr, loss_te)\n",
    "                    dbg_msg += '\\t\\tBest Loss Function : Train = {:.6f} | Test = {:.6f}, {:.6f} | Constr = {}\\n'.format(\n",
    "                        best_train_loss, best_test_loss, best_test_noconstr, best_constr_val.round(6))\n",
    "                    dbg_msg += '\\t\\tPer rollout loss train : {} | Loss test : {}\\n'.format(\n",
    "                        spec_data_tr[:, 1].round(6), spec_data_te[:, 1].round(6))\n",
    "                    dbg_msg += '\\t\\tPer rollout EQ Constraint Train : {} | Test : {}\\n'.format(\n",
    "                        spec_data_tr[:, 2].round(6), spec_data_te[:, 2].round(6))\n",
    "                    dbg_msg += '\\t\\tPer rollout INEQ Constraint Train : {} | Test : {}\\n'.format(\n",
    "                        spec_data_tr[:, 3].round(6), spec_data_te[:, 3].round(6))\n",
    "                    dbg_msg += '\\t\\tColocotion loss : {}\\n'.format(\n",
    "                        coloc_cte.round(8))\n",
    "                    tqdm.write(dbg_msg)\n",
    "\n",
    "                # Update the number of iteration\n",
    "                number_inner_iteration += 1\n",
    "\n",
    "                # If the patience periode has been violated, try to break\n",
    "                if patience > 0 and iter_since_best_param > patience:\n",
    "\n",
    "                    # Debug message\n",
    "                    tqdm.write('####################### EARLY STOPPING [{}][{}] #######################'.format(\n",
    "                        step, number_lagrange_update))\n",
    "                    tqdm.write('Best Loss Function : Train = {:.6f} | Test = {:.6f}, {:.6f}'.format(\n",
    "                        best_train_loss, best_test_loss, best_test_noconstr))\n",
    "\n",
    "                    # If there is no constraints then break\n",
    "                    if not train_with_constraints:\n",
    "                        break\n",
    "\n",
    "                    # Check if the constraints threshold has been attained\n",
    "                    if best_constr_val[1] < mParamsNN.pen_constr['tol_constraint_eq'] and best_constr_val[2] < mParamsNN.pen_constr['tol_constraint_ineq']:\n",
    "                        tqdm.write('Constraints satisfied: [eq =  {}], [ineq = {}]\\n'.format(\n",
    "                            best_constr_val[1], best_constr_val[2]))\n",
    "                        tqdm.write(\n",
    "                            '##############################################################################\\n')\n",
    "                        break\n",
    "\n",
    "                    # If the constraints threshold hasn't been violated, update the lagrangian and penalty coefficients\n",
    "                    m_pen_eq_k, m_pen_ineq_k, m_lagr_eq_k, m_lagr_ineq_k = \\\n",
    "                        update_lagrange(params, pen_eq_k=m_pen_eq_k, pen_ineq_sq_k=m_pen_ineq_k,\n",
    "                                        lagr_eq_k=m_lagr_eq_k, lagr_ineq_k=m_lagr_ineq_k, extra_args_colocation=(coloc_set, None, None))\n",
    "\n",
    "                    # Update the new params to be the best params\n",
    "                    params = best_params\n",
    "\n",
    "                    # UPdate the number of inner iteration since the last lagrangian terms update\n",
    "                    number_inner_iteration = 0\n",
    "\n",
    "                    # Update the number of lagrange update so far\n",
    "                    number_lagrange_update += 1\n",
    "\n",
    "                    # Reinitialize the optimizer\n",
    "                    opt_state = opt.init(params)\n",
    "\n",
    "                    # Some printing for debig\n",
    "                    tqdm.write('Update Penalty : [eq = {:.4f}, lag_eq = {:.4f}] | [ineq = {:.4f}, lag_ineq = {:.4f}]'.format(\n",
    "                        m_pen_eq_k, jnp.sum(jnp.abs(m_lagr_eq_k)), m_pen_ineq_k, jnp.sum(m_lagr_ineq_k)))\n",
    "\n",
    "            # Once the algorithm has converged, collect and save the data and params\n",
    "            dict_loss_per_seed[mParamsNN.seed_init] = dict_loss\n",
    "            final_log_dict[i] = dict_loss_per_seed\n",
    "            # Here we update the weights with the noise\n",
    "            key = jax.random.PRNGKey(0)\n",
    "            # print(best_params) # TODO: Remove this\n",
    "            best_params_noisy = {}\n",
    "            for it_key, it_value in best_params.items():\n",
    "                best_params_noisy[it_key] = {}\n",
    "                for w_or_b, w_or_b_value in it_value.items():\n",
    "                    if w_or_b == 'w':\n",
    "                        best_params_noisy[it_key][w_or_b] = add_gaussian_noise(key, w_or_b_value, scale=weight_noise)\n",
    "                    elif w_or_b == 'b':\n",
    "                        best_params_noisy[it_key][w_or_b] = add_gaussian_noise(key, w_or_b_value, scale=bias_noise)\n",
    "            dict_params_per_seed[mParamsNN.seed_init] = best_params_noisy\n",
    "            final_params_dict[i] = dict_params_per_seed\n",
    "\n",
    "            # Cretae the log\n",
    "            mLog = LearningLog(env_name=mSampleLog.env_name, env_extra_args=mSampleLog.env_extra_args, baseline=type_baseline,\n",
    "                               sampleLog=reducedSampleLog, nn_hyperparams=mParamsNN_list, loss_evol=final_log_dict, learned_weights=final_params_dict,\n",
    "                               data_set_file=data_set_file)\n",
    "\n",
    "            # Save the current information\n",
    "            mFile = open(out_file+'.pkl', 'wb')\n",
    "            pickle.dump(mLog, mFile)\n",
    "            mFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# An example of config file for the parameters used to train a single 
# neural network to predict the next state based on the current state and input signal 

# Gaussian weights noise, defaults to 0.1 if no value:
weight_noise: 0.0

# Gaussian biases noise, defaults to 0.1 if no value:
bias_noise: 0.0

# Gaussian constraints noise, defaults to 0.1 if no value:
constraint_noise: 0

# Choose if perform Gaussian Process denoising before training on data
perform_gp_denoising: False

# If perform_gp_denoising is True, otherwise ignore:
# Use Log Marginal Likelihood to find the best hyperparameters for the GP denoising
optimize_hyperparams: False 
# Don't square the noise in the calculations, so it doesn't end up even smaller
big_denoising: False
# The sigma_n (noise) that will be used for denoising if the hyperparameters are not optimized (optimize_hyperparams: False)

# If gradient_regularization is set to true, this value will be used as the maximum norm of noise
# TODO: Specify that no matter if the adv noise is for the whole trajectory or for one datapoint,
# the default_sigma_n is the norm of the noise of a datapoint.
default_sigma_n: 0.2
# Choose if adapt the loss function in order to do gradient regularization
gradient_regularization: True

# The file that contains the training and testing dataset
train_data_file: ./data/datatrain_dt0p01.pkl

# The file used to save the results of the training -> No extension
output_file: ./data/baseline_nosi

# An identifier to uniquely name the parameter of this NN
model_name: double_pendulum

# Optimizer and its parameters
optimizer:
  name: adam
  learning_rate_init: 0.01
  learning_rate_end: 0.001    # Decay the learning rate until the value
  weight_decay: 0.0001        # Weight decay coefficient
  grad_clip: 0.01             # Gradient clip coefficient
  # params:
    # learning_rate: 0.01
    
# The seed for randomness and reproducibility
seed: [101, 201]

# Batch size 
batch_size: 64

# L2 regularization penalty term
pen_l2: 0.0

# Regularization penalty for constraints (if present)
# pen_constr: [0.0, 1.1, 0.0, 1.1] # init penalty equality term, multiplicative update equality term, init penalty inequality term, multiplicative update inequality term
pen_constr:
  pen_eq_init: 0.001
  beta_eq: 1.5
  pen_ineq_init: 0.001
  beta_ineq: 1.5
  num_eq_constr: 4
  num_ineq_constr: 0
  tol_constraint_eq: 0.0001
  tol_constraint_ineq: 0.0001


# The baseline ODESolver algorithm
baseline: base

# Define the neural network params and its initialization -> This depends on the type of side information
# Example with no side information
nn_params:
  # Specify the side information
  type_sideinfo: 0 # 0 means None, 1 : Coriolis/Joints, 2: Coriolis/Joints + Actuator, 3: Coriolis/Joints + Part Actuator, 4: All except contact forces 

  vector_field:
    output_sizes: [256, 256]          # Specify the size of the hidden layers only
    activation: relu                  # Activation function
    b_init:
      initializer: Constant           # Initializer of the biais value
      params:
        constant: 0                   # arguments of Constant initlaizer
    w_init:
      initializer: RandomUniform      # Initializer of the weight values
      params:
        minval: -0.05
        maxval: 0.05

  # Define the remainder term neural network
  apriori_encl:
    output_sizes: [32,32,32]
    activation: relu
    b_init:
      initializer: Constant           # Initializer of the biais value
      params:
        constant: 0                   # arguments of Constant initlaizer
    w_init:
      initializer: RandomUniform      # Initializer of the weight values
      params:
        minval: -0.01
        maxval: 0.01

# Total number of iterations
num_gradient_iterations: 50000

# Frequence of printing information and saving in the file
freq_save: 500

# An integer that specifies if applying early stopping or not.
# Using early stopping criteria, patience specifies the number of step before deciding if we have the best solution or not
patience: 10

# Frequency at which to compute loss function on the training and testing
freq_accuracy: [1.0, 100, 100]
